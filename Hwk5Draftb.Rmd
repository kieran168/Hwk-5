---
title: "Lab 5"
font-family: 'Corbel'
output: github_document
---
  

<p style="color:rgb(182,18,27);font-family:corbel">Econ B2000, MA Econometrics</p>
<p style="color:rgb(182,18,27);font-family:corbel">Kevin R Foster, the Colin Powell School at the City College of New York, CUNY</p>
<p style="color:rgb(182,18,27);font-family:corbel">Fall 2020</p>
  
For this lab, we improve some of our regression models to explain wages.

Work with a group to prepare a 4-min presentation by one of the group members about their experiment process and results. You get 45 min to prepare.

Build on the previous lab in creating useful models.

Concentrate on a smaller subset than previous. For instance if you wanted to look at wages for Hispanic women with at least a college degree, you might use

```{r eval = FALSE}
setwd("~/CCNY/Econometrics/Homework/Draft Hwk Files")
load("acs2017_ny_data.RData")
attach(acs2017_ny)
healthcare <- (IND == "7970" | IND == "7980" | IND == "7990" | IND == "8070" | IND == "8080" | IND == "8090" | IND == "8170" | IND == "8180" | IND == "8190" | IND == "8270")
use_varb <- (AGE >= 25) & (AGE <= 70) & (LABFORCE == 2) & (WKSWORK2 > 4) & (UHRSWORK >= 35) & (female == 1) & ((educ_college == 1) | (educ_advdeg == 1)) & healthcare
dat_med <- subset(acs2017_ny, use_varb) # 
detach()
```
```{r}
attach(dat_med)
summary(dat_med)
detach(dat_med)
```
```{r}
# creating different objects for work departure time
# want to create different objects within industry to subset office workers, hospitals, nursing facilities etc
attach(dat_med)
DEPARTS
length(DEPARTS)
graveyard <- ((DEPARTS >= 0) & (DEPARTS <= 459))
morning <- ((DEPARTS >= 500) & (DEPARTS <=930))
daytime <- ((DEPARTS >= 931) & (DEPARTS <=1700))
evening <- ((DEPARTS >= 1701) & (DEPARTS <=2359))
summary(graveyard)
summary(morning)
summary(daytime)
summary(evening)
table(morning)
deptime <- factor((1*evening + 2*morning + 3*daytime + 4*graveyard), levels = c(1, 2, 3, 4), labels = c( "Night", "Grave", "Morn", "Mid"))
table(deptime)
sum(morning+daytime)
detach()
```
```{r}
summary(INCWAGE)
```
Try a regression with age and age-squared in addition to your other controls, something like this:

```{r}
attach(dat_med)
lma <- lm((INCWAGE ~ AGE + I(AGE^2) + educ_college))
lmdep <- lm((INCWAGE ~ AGE + I(AGE^2) + educ_college + deptime))
# summary(lma)
summary(lmdep)
plot(lma)
require(stargazer)
suppressWarnings(stargazer(lma, type = "text"))
```
```{r}
NNobs <- length(INCWAGE)
set.seed(12345)
graph_obs <- (runif(NNobs) < 0.5)
med_graph <-subset(dat_med, graph_obs) 
plot(INCWAGE ~ jitter(AGE, factor = 0.1), pch = 16, col = rgb(0.5, 0.5, 0.5, alpha = 0.2), ylim = c(0,300000), data = med_graph)
require(AER)
medpredict <- data.frame(AGE = 25:70, female = 1, educ_college = 1, educ_advdeg = 1)
medpredict$yhat <- predict(lma, newdata = medpredict)
lines(yhat ~ AGE, data = medpredict)
max(medpredict$yhat) # peak predicted wage
```
```{r}
plot(INCWAGE ~ jitter(AGE, factor = 0.1), pch = 16, col = rgb(0.5, 0.5, 0.5, alpha = 0.2), ylim = c(0,300000), data = med_graph)
require(AER)
medpredictb <- data.frame(AGE = 25:70, female = 1, educ_college = 0, educ_advdeg = 1)
medpredictb$yhat <- predict(lma, newdata = medpredictb)
lines(yhat ~ AGE, data = medpredictb, col = "blue")
max(medpredictb$yhat) # peak predicted wage
```
What is the peak of predicted wage? What if you add higher order polynomials of age, such as $Age^3$ or $Age^4$? 
```{r}
# sets up lm with polynomials included
attach(dat_med)
lmpolys <- lm((INCWAGE ~ AGE + I(AGE^2) + I(AGE^3) + I(AGE^4) + I(AGE^5) + educ_college))
# summary(lma)
plot(lmpolys)
require(stargazer)
suppressWarnings(stargazer(lmpolys, type = "text"))
```
```{r}
# plots line on plot - peak wages AREN'T particularly different from those of the Aged^2 lm
plot(INCWAGE ~ jitter(AGE, factor = 1), pch = 16, col = rgb(0.5, 0.5, 0.5, alpha = 0.2), ylim = c(0,300000), data = med_graph)
medpredict$yhatploys <- predict(lmpolys, newdata = medpredict)
lines(yhatploys ~ AGE, data = medpredict)
max(medpredict) # peak predicted wage
```
```{r}
# Hypothesis test of higher order polynomials - jointly significant
coeftest(lmpolys,vcovHC)
require(car)
# or
linearHypothesis(lmpolys, "I(AGE^2) + I(AGE^3) + I(AGE^4) + I(AGE^5) = 0", test="F")
```

Do a hypothesis test of whether all of those higher-order polynomial terms are jointly significant. Describe the pattern of predicted wage as a function of age. 

```{r}
# The pattern of predicted wage trends upward until age reaches the mid 50s, plateaus until approximately 57 years old then trends down towards 70 years old.
```

What if you used $log(Age)$? (And why would polynomials in $log(Age)$ be useless? Experiment.)

```{r}
lmlog <- lm((INCWAGE ~ AGE + log(AGE) + educ_college))
# plot(lmlog)
plot(INCWAGE ~ jitter(AGE, factor = 1), pch = 16, col = rgb(0.5, 0.5, 0.5, alpha = 0.2), ylim = c(0,300000), data = med_graph)
medpredict$yhatlog <- predict(lmlog, newdata = medpredict)
lines(yhatlog ~ AGE, data = medpredict)
max(medpredict) # peak predicted wage
# using log(AGE) smooths the curve towards the upper end of the age range. We shouldn't use polynomials as they will undo the impact of the log function.
```

Recall about how dummy variables work. If you added educ_hs in a regression using the subset given above, what would that do? (Experiment, if you aren't sure.) What is interpretation of coefficient on *educ_college* in that subset? What would happen if you put both *educ_college* and *educ_advdeg* into a regression? Are your other dummy variables in the regression working sensibly with your selection criteria?

```{r}
# Adding educ_hs in our particular subset will render it as omitted (we haven't included it as a variable in the subset). If we add the educ_advdeg variable, it will also be rendered as omitted, so R can run a comparison.
lmedu
lmedua <- lm((INCWAGE ~ AGE + I(AGE^2) + educ_advdeg + educ_college))
lmedua
# if we change the position of the educ_advdeg variable to place it BEFORE the educ_college variable, R drops the LAST variable from the regression. What is interesting is that whichever variable is included, the coefficient is the absolute same value but has a negative value when educ_college is the included variable.
```
Why don't we use polynomial terms of dummy variables? Experiment.

```{r}
# We don't use polynomials of dummy variables as they have no effect on the regression - 1^x = 1; 0^x = 0.
```
What is the predicted wage, from your model, for a few relevant cases? Do those seem reasonable?
```{r}
# check wage as a function of age for lma and medpredict
# we can also change the medpredict objects to use the poly regression or the log regression
medpredictc <- data.frame(AGE = 25:70, female = 1, educ_college = 1, educ_advdeg = 0)
medpredictc$yhat <- predict(lma, newdata = medpredictc)
medpredict
medpredictb
medpredictc
```

What is difference in regression from using log wage as the dependent variable? Compare the pattern of predicted values from the two models (remember to take exp() of the predicted value, where the dependent is log wage). Discuss.

```{r}
# included all 3 regressions into medpredict in prior chunks to allow for easy comparison in the table
medpredict
mean(medpredict$yhat)
mean(medpredict$yhatlog)
```

Try some interactions, like this,
```{r}
# interactions
lminter <- lm(INCWAGE ~ AGE + I(AGE^2) + educ_college + I(educ_college*AGE) + I(educ_college*(AGE^2)))
lminter
lminterb <- lm(INCWAGE ~ AGE + I(AGE^2) + educ_college + I(educ_college*AGE) + educ_advdeg + I(educ_advdeg*AGE))
lminterb
```
and explain those outputs (different peaks for different groups).

What are the other variables you are using in your regression? Do they have the expected signs and patterns of significance? Explain if there is a plausible causal link from X variables to Y and not the reverse. Explain your results, giving details about the estimation, some predicted values, and providing any relevant graphics. Impress.